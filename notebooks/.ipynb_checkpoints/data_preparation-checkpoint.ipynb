{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Data Preparation\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "\n",
    "This notebook contains the common data loading and preparation steps:\n",
    "- load data from the input CSV\n",
    "- fix missing values\n",
    "- clean up anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and variables\n",
    "Imports and variable definitions that are common to the entire notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2019.6.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 20.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 20.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "# common imports\n",
    "import zipfile\n",
    "import time\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import os\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    ''' open config file with name config_file that contains parameters\n",
    "    for this module and return Python object\n",
    "\n",
    "    Args:\n",
    "        config_file: filename containing config parameters\n",
    "\n",
    "    Returns:\n",
    "        config: Python dictionary with config parms from config file - dictionary\n",
    "\n",
    "\n",
    "    '''\n",
    "    current_path = os.getcwd()\n",
    "    print(\"current directory is: \" + current_path)\n",
    "\n",
    "    path_to_yaml = os.path.join(current_path, config_file)\n",
    "    print(\"path_to_yaml \" + path_to_yaml)\n",
    "    try:\n",
    "        with open(path_to_yaml, 'r') as c_file:\n",
    "            config = yaml.safe_load(c_file)\n",
    "        return config\n",
    "    except Exception as error:\n",
    "        print('Error reading the config file ' + str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "- ingest CVS into a Pandas dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    ''' get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "\n",
    "    '''\n",
    "    rawpath = os.getcwd()\n",
    "    # data is in a directory called \"data\" that is a sibling to the directory\n",
    "    # containing the notebook\n",
    "    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(dataset,textcols,continuouscols,collist,text_default,continuous_default,categorical_default):\n",
    "    ''' replace missing values with placeholders by column type\n",
    "    \n",
    "    Args:\n",
    "        dataset: dataframe in which missing values being processed\n",
    "        textcols: list of text columns\n",
    "        continuouscols: list of continuous columns\n",
    "        collist: list of categorical columns\n",
    "        text_default: replacement value for missing values in text columns\n",
    "        continous_default: replacement value for missing values in continuous columns\n",
    "        categorical_default: replacement value for missing values in categorical columns\n",
    "\n",
    "    Returns:\n",
    "        dataset: dataframe with missing values replaced\n",
    "\n",
    "    '''\n",
    "    logging.debug(\"before mv\")\n",
    "    for col in collist:\n",
    "        dataset[col].fillna(value=categorical_default, inplace=True)\n",
    "    for col in continuouscols:\n",
    "        dataset[col].fillna(value=continuous_default,inplace=True)\n",
    "    for col in textcols:\n",
    "        dataset[col].fillna(value=text_default, inplace=True)\n",
    "    return (dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframe\n",
    "- load pickled dataframe\n",
    "- show info about the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-55-be3c09a46898>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-55-be3c09a46898>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    file_name = os.path.join(path,pickled_input_dataframe])\u001b[0m\n\u001b[1;37m                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def ingest_data(path,input_csv,pickled_input_dataframe,save_raw_dataframe,load_from_scratch):\n",
    "    ''' load data into dataframe\n",
    "    Args:\n",
    "        path: path containing input file\n",
    "        input_csv: input file name\n",
    "        pickled_input_dataframe: pickled version of input file\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "    '''\n",
    "    if load_from_scratch:\n",
    "        unpickled_df = pd.read_csv(os.path.join(path,input_csv)) \n",
    "        if save_raw_dataframe:\n",
    "            file_name = os.path.join(path,pickled_input_dataframe)\n",
    "            print(\"file_name is \",file_name)\n",
    "            df.to_pickle(file_name)\n",
    "    else:\n",
    "        unpickled_df = pd.read_pickle(os.path.join(path,pickled_input_dataframe))\n",
    "        logging.debug(\"reloader done\")\n",
    "    return(unpickled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General cleanup\n",
    "- correct types for Route and Vehicle\n",
    "- fill missing values\n",
    "- create report-date-time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset incorporated some anomalies in the 2019 data, including:\n",
    "# extraneous Incident ID in April 2019 tab\n",
    "# Gap and Delay columns in April and June 2019 tabs for what had otherwise been called Min Gap and Min Delay\n",
    "# this function cleans up these anomalies\n",
    "def fix_anomalous_columns(df):\n",
    "    # for rows where there is NaN in the Min Delay or Min Gap columns, copy over value from Delay or Gap\n",
    "    # df.Temp_Rating.fillna(df.Farheit, inplace=True)\n",
    "    df['Min Delay'].fillna(df['Delay'], inplace=True)\n",
    "    df['Min Gap'].fillna(df['Gap'], inplace=True)\n",
    "    # now that the useful values have been copied from Delay and Gap, remove them\n",
    "    del df['Delay']\n",
    "    del df['Gap']\n",
    "    # remove Incident ID column - it's extraneous\n",
    "    del df['Incident ID']\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_time(date_time_value,time_value):\n",
    "    ''' given a datetime replace the time portion '''\n",
    "     \n",
    "    date_time_value = date_time_value.replace(hour=time_value.hour,minute=time_value.minute,second=time_value.minute)\n",
    "    return(date_time_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_cleanup(df):\n",
    "    # ensure Route and Vehicle are strings, not numeric\n",
    "    df['Route'] = df['Route'].astype(str)\n",
    "    df['Vehicle'] = df['Vehicle'].astype(str)\n",
    "    # remove extraneous characters left from Vehicle values being floats\n",
    "    df['Vehicle'] = df['Vehicle'].str[:-2]\n",
    "    # tactical definition of categories\n",
    "    allcols,textcols,continuouscols,timecols,collist = define_feature_categories(df)\n",
    "    # fill in missing values\n",
    "    df.isnull().sum(axis = 0)\n",
    "    df = fix_anomalous_columns(df)\n",
    "    df = fill_missing(df,allcols,textcols,continuouscols,timecols,collist)\n",
    "    # create new column combining date + time (needed for resampling) and make it the index\n",
    "    df['Report Date Time'] = df.apply(lambda x: replace_time(x['Report Date'], x['Time']), axis=1)\n",
    "    df.index = df['Report Date Time']\n",
    "    # return the updated dataframe along with the column category lists\n",
    "    return(df,allcols,textcols,continuouscols,timecols,collist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master cell\n",
    "This cell contains calls to the other functions in this notebook to complete the data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "path = get_path()\n",
    "print(\"path is \",path)\n",
    "config = get_config('data_preparation_config.yml')\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.warning(\"logging check\")\n",
    "# load route direction and delay data datframes\n",
    "df = ingest_data(path,config['file_names']['input_csv'],config['file_names']['pickled_input_dataframe'],config['general']['save_raw_dataframe'],config['general']['load_from_scratch'])\n",
    "# iterate through columns to get basic information\n",
    "for col in list(df):\n",
    "    print(\"Missing values in \",col,\" \",str(df[col].isna().sum()))\n",
    "    print(\"Distinct values \",str(df[col].nunique()))\n",
    "df = fill_missing(df,config['text'],config['continuous'],config['categorical'],config['general']['text_default'],config['general']['continuous_default'],config['general']['categorical_default'])\n",
    "df.head()\n",
    "'''\n",
    "collist = config['categorical']\n",
    "textcols = config['text']\n",
    "continuouscols = config['continuous']\n",
    "excludefromcolist = config['excluded']\n",
    "\n",
    "'''\n",
    "\n",
    "if config['general']['save_transformed_dataframe']:\n",
    "    print(\"path is \",path)\n",
    "    file_name = os.path.join(path,config['file_names']['pickled_output_dataframe'])\n",
    "    print(\"file_name is \",file_name)\n",
    "    df.to_pickle(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
