{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Deep Learning model training - HELP VERSION\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "This notebook contains the code to train the model from the dataset prepared in the airbnb_data_preparation notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links to key parts of the notebook <a name='linkanchor' />\n",
    "<a href=#ingestdash>Ingest data</a>\n",
    "\n",
    "<a href=#buildpipe>Build pipeline</a>\n",
    "\n",
    "<a href=#modelfit>Define and fit model</a>\n",
    "\n",
    "<a href=#reload>Reload saved model and weights</a>\n",
    "\n",
    "<a href=#confusionmatrix>Confusion matrix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and global variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common imports\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "import pydotplus\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "# DSX code to import uploaded documents\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import yaml\n",
    "import math\n",
    "import sys\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "#model libraries\n",
    "from tensorflow.keras.metrics import Accuracy, Recall, Precision\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#from tf.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "#import datetime\n",
    "#from datetime import date\n",
    "from sklearn import metrics\n",
    "# import pipeline libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from custom_classes import encode_categorical\n",
    "from custom_classes import prep_for_keras_input\n",
    "from custom_classes import fill_empty\n",
    "from custom_classes import encode_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_path = os.getcwd()\\nprint(\"current directory is: \"+current_path)\\n\\npath_to_yaml = os.path.join(current_path, \\'model_training_config.yml\\')\\nprint(\"path_to_yaml \"+path_to_yaml)\\ntry:\\n    with open (path_to_yaml, \\'r\\') as c_file:\\n        config = yaml.safe_load(c_file)\\nexcept Exception as e:\\n    print(\\'Error reading the config file\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load config file\n",
    "'''\n",
    "current_path = os.getcwd()\n",
    "print(\"current directory is: \"+current_path)\n",
    "\n",
    "path_to_yaml = os.path.join(current_path, 'model_training_config.yml')\n",
    "print(\"path_to_yaml \"+path_to_yaml)\n",
    "try:\n",
    "    with open (path_to_yaml, 'r') as c_file:\n",
    "        config = yaml.safe_load(c_file)\n",
    "except Exception as e:\n",
    "    print('Error reading the config file')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrepeatable_run = config[\\'test_parms\\'][\\'repeatable_run\\']\\n# fix seeds to get identical results on mulitiple runs\\nif repeatable_run:\\n    from numpy.random import seed\\n    seed(4)\\n    tf.random.set_seed(7)\\n\\n\\ntestproportion = config[\\'test_parms\\'][\\'testproportion\\'] # proportion of data reserved for test set\\ntrainproportion = config[\\'test_parms\\'][\\'trainproportion\\'] # proportion of non-test data dedicated to training (vs. validation)\\nget_test_train_acc = config[\\'test_parms\\'][\\'get_test_train_acc\\']\\nverboseout = config[\\'general\\'][\\'verboseout\\']\\nincludetext = config[\\'general\\'][\\'includetext\\'] # switch to determine whether text columns are included in the model\\nsave_model_plot = config[\\'general\\'][\\'save_model_plot\\'] # switch to determine whether to generate plot with plot_model\\ntensorboard_callback = config[\\'general\\'][\\'tensorboard_callback\\'] # switch to determine if tensorboard callback defined\\n\\npresaved = config[\\'general\\'][\\'presaved\\']\\nsavemodel = config[\\'general\\'][\\'savemodel\\']\\npicklemodel = config[\\'general\\'][\\'picklemodel\\']\\nhctextmax = config[\\'general\\'][\\'hctextmax\\']\\nmaxwords = config[\\'general\\'][\\'maxwords\\']\\ntextmax = config[\\'general\\'][\\'textmax\\']\\n\\ntargetthresh = config[\\'general\\'][\\'targetthresh\\']\\ntargetcontinuous = config[\\'general\\'][\\'targetcontinuous\\']\\ntarget_col = config[\\'general\\'][\\'target_col\\']\\n\\n#time of day thresholds\\ntime_of_day = {\\'overnight\\':{\\'start\\':0,\\'end\\':5},\\'morning_rush\\':{\\'start\\':5,\\'end\\':10},\\n              \\'midday\\':{\\'start\\':10,\\'end\\':15},\\'aft_rush\\':{\\'start\\':15,\\'end\\':19},\\'evening\\':{\\'start\\':19,\\'end\\':24}}\\n\\n\\n\\nemptythresh = config[\\'general\\'][\\'emptythresh\\']\\nzero_weight = config[\\'general\\'][\\'zero_weight\\']\\none_weight = config[\\'general\\'][\\'one_weight\\']\\none_weight_offset = config[\\'general\\'][\\'one_weight_offset\\']\\npatience_threshold = config[\\'general\\'][\\'patience_threshold\\']\\n\\n\\n# modifier for saved model elements\\nmodifier = config[\\'general\\'][\\'modifier\\']\\n\\n# control whether training controlled by early stop\\nearly_stop = True\\n\\n# default hyperparameter values\\nlearning_rate = config[\\'hyperparameters\\'][\\'learning_rate\\']\\ndropout_rate = config[\\'hyperparameters\\'][\\'dropout_rate\\']\\nl2_lambda = config[\\'hyperparameters\\'][\\'l2_lambda\\']\\nloss_func = config[\\'hyperparameters\\'][\\'loss_func\\']\\noutput_activation = config[\\'hyperparameters\\'][\\'output_activation\\']\\nbatch_size = config[\\'hyperparameters\\'][\\'batch_size\\']\\nepochs = config[\\'hyperparameters\\'][\\'epochs\\']\\n\\n# date values\\ndate_today = datetime.now()\\nprint(\"date today\",date_today)\\n\\n# pickled original dataset and post-preprocessing dataset\\npickled_data_file = config[\\'general\\'][\\'pickled_data_file\\']\\npickled_dataframe = config[\\'general\\'][\\'pickled_dataframe\\']\\n\\n# experiment parameter\\n\\ncurrent_experiment = config[\\'test_parms\\'][\\'current_experiment\\']\\n\\n# load lists of column categories\\ncollist = config[\\'categorical\\']\\ntextcols = config[\\'text\\']\\ncontinuouscols = config[\\'continuous\\']\\nexcludefromcolist = config[\\'excluded\\']\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load parameters\n",
    "'''\n",
    "repeatable_run = config['test_parms']['repeatable_run']\n",
    "# fix seeds to get identical results on mulitiple runs\n",
    "if repeatable_run:\n",
    "    from numpy.random import seed\n",
    "    seed(4)\n",
    "    tf.random.set_seed(7)\n",
    "\n",
    "\n",
    "testproportion = config['test_parms']['testproportion'] # proportion of data reserved for test set\n",
    "trainproportion = config['test_parms']['trainproportion'] # proportion of non-test data dedicated to training (vs. validation)\n",
    "get_test_train_acc = config['test_parms']['get_test_train_acc']\n",
    "verboseout = config['general']['verboseout']\n",
    "includetext = config['general']['includetext'] # switch to determine whether text columns are included in the model\n",
    "save_model_plot = config['general']['save_model_plot'] # switch to determine whether to generate plot with plot_model\n",
    "tensorboard_callback = config['general']['tensorboard_callback'] # switch to determine if tensorboard callback defined\n",
    "\n",
    "presaved = config['general']['presaved']\n",
    "savemodel = config['general']['savemodel']\n",
    "picklemodel = config['general']['picklemodel']\n",
    "hctextmax = config['general']['hctextmax']\n",
    "maxwords = config['general']['maxwords']\n",
    "textmax = config['general']['textmax']\n",
    "\n",
    "targetthresh = config['general']['targetthresh']\n",
    "targetcontinuous = config['general']['targetcontinuous']\n",
    "target_col = config['general']['target_col']\n",
    "\n",
    "#time of day thresholds\n",
    "time_of_day = {'overnight':{'start':0,'end':5},'morning_rush':{'start':5,'end':10},\n",
    "              'midday':{'start':10,'end':15},'aft_rush':{'start':15,'end':19},'evening':{'start':19,'end':24}}\n",
    "\n",
    "\n",
    "\n",
    "emptythresh = config['general']['emptythresh']\n",
    "zero_weight = config['general']['zero_weight']\n",
    "one_weight = config['general']['one_weight']\n",
    "one_weight_offset = config['general']['one_weight_offset']\n",
    "patience_threshold = config['general']['patience_threshold']\n",
    "\n",
    "\n",
    "# modifier for saved model elements\n",
    "modifier = config['general']['modifier']\n",
    "\n",
    "# control whether training controlled by early stop\n",
    "early_stop = True\n",
    "\n",
    "# default hyperparameter values\n",
    "learning_rate = config['hyperparameters']['learning_rate']\n",
    "dropout_rate = config['hyperparameters']['dropout_rate']\n",
    "l2_lambda = config['hyperparameters']['l2_lambda']\n",
    "loss_func = config['hyperparameters']['loss_func']\n",
    "output_activation = config['hyperparameters']['output_activation']\n",
    "batch_size = config['hyperparameters']['batch_size']\n",
    "epochs = config['hyperparameters']['epochs']\n",
    "\n",
    "# date values\n",
    "date_today = datetime.now()\n",
    "print(\"date today\",date_today)\n",
    "\n",
    "# pickled original dataset and post-preprocessing dataset\n",
    "pickled_data_file = config['general']['pickled_data_file']\n",
    "pickled_dataframe = config['general']['pickled_dataframe']\n",
    "\n",
    "# experiment parameter\n",
    "\n",
    "current_experiment = config['test_parms']['current_experiment']\n",
    "\n",
    "# load lists of column categories\n",
    "collist = config['categorical']\n",
    "textcols = config['text']\n",
    "continuouscols = config['continuous']\n",
    "excludefromcolist = config['excluded']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths required\n",
    "\n",
    "def get_path():\n",
    "    '''get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data files\n",
    "    '''\n",
    "\n",
    "\n",
    "def get_pipeline_path():\n",
    "    '''get the path for data files\n",
    "    \n",
    "    Returns:\n",
    "        path: path for pipeline files\n",
    "    '''\n",
    "\n",
    "\n",
    "def get_model_path():\n",
    "    '''get the path for data files\n",
    "    \n",
    "    Returns:\n",
    "        path: path for model files\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_experiment_parameters(experiment_number, count_no_delay, count_delay):\n",
    "    ''' set the appropriate parameters for the experiment \n",
    "    Args:\n",
    "        experiment_number: filename containing config parameters\n",
    "        count_no_delay: count of negative outcomes in the dataset\n",
    "        count_delay: count of positive outcomes in the dataset\n",
    "\n",
    "    Returns:\n",
    "        early_stop: whether the experiment includes an early stop callback\n",
    "        one_weight: weight applied to positive outcomes\n",
    "        epochs: number of epochs in the experiment\n",
    "        es_monitor: performance measurement tracked in callbacks\n",
    "        es_mod: direction of performance being tracked in callbacks\n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data and create refactored dataframe <a name='ingestdash' />\n",
    "- Ingest data for route information and delay information\n",
    "- Create refactored dataframe with one row per route / direction / timeslot combination\n",
    "\n",
    "\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path):\n",
    "    '''load list of valid routes and directions into dataframe\n",
    "    Args:\n",
    "        path: path for data files\n",
    "    \n",
    "    Returns:\n",
    "        merged_data: dataframe loaded from pickle file\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_merged_data(merged_data,target_col):\n",
    "    '''add derived columns to merged_data dataframe\n",
    "    Args:\n",
    "        merged_data: input dataframe\n",
    "        target_col: column that is the target\n",
    "    \n",
    "    Returns:\n",
    "        merged_data: dataframe with derived columns added\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Prep Cell\n",
    "Contains calls to functions to load data, prep input dataframes, and create refactored dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = get_path()\\nprint(\"path is\",path)\\n# load route direction and delay data datframes\\nmerged_data = ingest_data(path)\\nmerged_data = prep_merged_data(merged_data,target_col)\\nprint(\"shape of pre refactored dataset\", merged_data.shape)\\n#merged_data[\\'year\\'].value_counts()\\n#merged_data.groupby([\\'Route\\',\\'Direction\\']).size().reset_index().rename(columns={0:\\'count\\'}).tail(50)\\n# create refactored dataframe with one row for each route / direction / timeslot combination\\nprint(\"shape of refactored dataset\", merged_data.shape)\\ncount_no_delay = merged_data[merged_data[\\'target\\']==0].shape[0]\\ncount_delay = merged_data[merged_data[\\'target\\']==1].shape[0]\\nprint(\"count under mean \",count_no_delay)\\nprint(\"count over mean \",count_delay)\\n# define parameters for the current experiment\\nexperiment_number = current_experiment\\nearly_stop, one_weight, epochs,es_monitor,es_mode = set_experiment_parameters(experiment_number, count_no_delay, count_delay)\\nprint(\"early_stop is \",early_stop)\\nprint(\"one_weight is \",one_weight)\\nprint(\"epochs is \",epochs)\\nprint(\"es_monitor is \",es_monitor)\\nprint(\"es_mode is \",es_mode)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master calls\n",
    "# get the path for data files\n",
    "\n",
    "'''\n",
    "path = get_path()\n",
    "print(\"path is\",path)\n",
    "# load route direction and delay data datframes\n",
    "merged_data = ingest_data(path)\n",
    "merged_data = prep_merged_data(merged_data,target_col)\n",
    "print(\"shape of pre refactored dataset\", merged_data.shape)\n",
    "#merged_data['year'].value_counts()\n",
    "#merged_data.groupby(['Route','Direction']).size().reset_index().rename(columns={0:'count'}).tail(50)\n",
    "# create refactored dataframe with one row for each route / direction / timeslot combination\n",
    "print(\"shape of refactored dataset\", merged_data.shape)\n",
    "count_no_delay = merged_data[merged_data['target']==0].shape[0]\n",
    "count_delay = merged_data[merged_data['target']==1].shape[0]\n",
    "print(\"count under mean \",count_no_delay)\n",
    "print(\"count over mean \",count_delay)\n",
    "# define parameters for the current experiment\n",
    "experiment_number = current_experiment\n",
    "early_stop, one_weight, epochs,es_monitor,es_mode = set_experiment_parameters(experiment_number, count_no_delay, count_delay)\n",
    "print(\"early_stop is \",early_stop)\n",
    "print(\"one_weight is \",one_weight)\n",
    "print(\"epochs is \",epochs)\n",
    "print(\"es_monitor is \",es_monitor)\n",
    "print(\"es_mode is \",es_mode)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define training, validation, and test subsets of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_validation_test(dataset):\n",
    "    '''get training and test data set\n",
    "    Args:\n",
    "        dataset: input dataframe\n",
    "    \n",
    "    Returns:\n",
    "        dtrain: training subset of dataset\n",
    "        dvalid: validation subset of dataset\n",
    "        dtest: test subset of dataset\n",
    "    '''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Pipeline <a name='buildpipe' />\n",
    "\n",
    "Create pipeline objects to perform final data preparation steps for training and inference.\n",
    "\n",
    "Note that cleanup on the training dataset is completed upstream in the [data cleanup notebook](https://github.com/ryanmark1867/end_to_end_deep_learning_liveproject/blob/master/notebooks/data_cleanup.ipynb). \n",
    "- The pipelines only accomplish the subset of preparation that is required for both training and inference\n",
    "- Because the scoring data coming in for inference is forced by the web deployment to avoid the invalid values that the data cleanup notebook deals with, the pipelines don't have to deal with those problems.\n",
    "\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# build fully qualified names for the files for saving the pipelines\\npipeline_path = get_pipeline_path()\\npipeline1_file_name = os.path.join(pipeline_path,\\'sc_delay_pipleline\\'+modifier+\\'.pkl\\')\\npipeline2_file_name = os.path.join(pipeline_path,\\'sc_delay_pipleline_keras_prep\\'+modifier+\\'.pkl\\')\\n\\n# define column lists:\\n# collist,continuouscols,textcols = def_col_lists()\\n\\n# create objects of the pipeline classes\\nfe = fill_empty()\\nec = encode_categorical()\\npk = prep_for_keras_input()\\npk_valid = prep_for_keras_input()\\npk_test = prep_for_keras_input()\\n\\n# need to implement the pipeline in two parts:\\n# 1. fill empty + encode categoricals\\n# 2. prep for Keras\\n# because part 1 needs to be applied to the entire dataset and part 2 to the individual train, validate, and test sets\\n\\n\\nsc_delay_pipeline = Pipeline([(\\'fill_empty\\',fe),(\\'encode_categorical\\',ec)])\\n# need to have distinct pipeline objects for each subset of the dataset: train, validated and test\\nsc_delay_pipeline_keras_prep = Pipeline([(\\'prep_for_keras\\',pk)])\\nsc_delay_pipeline_keras_prep_valid = Pipeline([(\\'prep_for_keras\\',pk_valid)])\\nsc_delay_pipeline_keras_prep_test = Pipeline([(\\'prep_for_keras\\',pk_test)])\\n\\n\\n\\n# provide the value for each parameter of each of the pipeline classes\\n\\nsc_delay_pipeline.set_params(fill_empty__collist = collist, fill_empty__continuouscols = continuouscols,\\n                            fill_empty__textcols = textcols,encode_categorical__col_list = collist)\\nsc_delay_pipeline_keras_prep.set_params(prep_for_keras__collist = collist,\\n                            prep_for_keras__continuouscols = continuouscols,\\n                            prep_for_keras__textcols = textcols)\\nsc_delay_pipeline_keras_prep_valid.set_params(prep_for_keras__collist = collist,\\n                            prep_for_keras__continuouscols = continuouscols,\\n                            prep_for_keras__textcols = textcols)\\nsc_delay_pipeline_keras_prep_test.set_params(prep_for_keras__collist = collist,\\n                            prep_for_keras__continuouscols = continuouscols,\\n                            prep_for_keras__textcols = textcols)\\n\\n# fit the input dataset to the pipeline\\n\\n# first fit the first segment of pipeline on the whole dataset\\nX = sc_delay_pipeline.fit_transform(merged_data)\\nmax_dict = ec.max_dict\\n# then split dataset\\ndump(sc_delay_pipeline, open(pipeline1_file_name,\\'wb\\'))\\ndump(sc_delay_pipeline_keras_prep, open(pipeline2_file_name,\\'wb\\'))\\ndtrain, dvalid, test = get_train_validation_test(X)\\n# then apply second portion of pipeline to each subset\\n# need to have a distinct object for each to prevent first object impacting others\\n\\nX_train_list = sc_delay_pipeline_keras_prep.fit_transform(dtrain)\\nX_valid_list = sc_delay_pipeline_keras_prep_valid.fit_transform(dvalid)\\nX_test_list = sc_delay_pipeline_keras_prep_test.fit_transform(test)\\n\\nprint(\"keras variables defined\")\\nprint(\"X_train_list\",X_train_list)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master block to invoke pipeline\n",
    "\n",
    "'''\n",
    "\n",
    "# build fully qualified names for the files for saving the pipelines\n",
    "pipeline_path = get_pipeline_path()\n",
    "pipeline1_file_name = os.path.join(pipeline_path,'sc_delay_pipleline'+modifier+'.pkl')\n",
    "pipeline2_file_name = os.path.join(pipeline_path,'sc_delay_pipleline_keras_prep'+modifier+'.pkl')\n",
    "\n",
    "# define column lists:\n",
    "# collist,continuouscols,textcols = def_col_lists()\n",
    "\n",
    "# create objects of the pipeline classes\n",
    "fe = fill_empty()\n",
    "ec = encode_categorical()\n",
    "pk = prep_for_keras_input()\n",
    "pk_valid = prep_for_keras_input()\n",
    "pk_test = prep_for_keras_input()\n",
    "\n",
    "# need to implement the pipeline in two parts:\n",
    "# 1. fill empty + encode categoricals\n",
    "# 2. prep for Keras\n",
    "# because part 1 needs to be applied to the entire dataset and part 2 to the individual train, validate, and test sets\n",
    "\n",
    "\n",
    "sc_delay_pipeline = Pipeline([('fill_empty',fe),('encode_categorical',ec)])\n",
    "# need to have distinct pipeline objects for each subset of the dataset: train, validated and test\n",
    "sc_delay_pipeline_keras_prep = Pipeline([('prep_for_keras',pk)])\n",
    "sc_delay_pipeline_keras_prep_valid = Pipeline([('prep_for_keras',pk_valid)])\n",
    "sc_delay_pipeline_keras_prep_test = Pipeline([('prep_for_keras',pk_test)])\n",
    "\n",
    "\n",
    "\n",
    "# provide the value for each parameter of each of the pipeline classes\n",
    "\n",
    "sc_delay_pipeline.set_params(fill_empty__collist = collist, fill_empty__continuouscols = continuouscols,\n",
    "                            fill_empty__textcols = textcols,encode_categorical__col_list = collist)\n",
    "sc_delay_pipeline_keras_prep.set_params(prep_for_keras__collist = collist,\n",
    "                            prep_for_keras__continuouscols = continuouscols,\n",
    "                            prep_for_keras__textcols = textcols)\n",
    "sc_delay_pipeline_keras_prep_valid.set_params(prep_for_keras__collist = collist,\n",
    "                            prep_for_keras__continuouscols = continuouscols,\n",
    "                            prep_for_keras__textcols = textcols)\n",
    "sc_delay_pipeline_keras_prep_test.set_params(prep_for_keras__collist = collist,\n",
    "                            prep_for_keras__continuouscols = continuouscols,\n",
    "                            prep_for_keras__textcols = textcols)\n",
    "\n",
    "# fit the input dataset to the pipeline\n",
    "\n",
    "# first fit the first segment of pipeline on the whole dataset\n",
    "X = sc_delay_pipeline.fit_transform(merged_data)\n",
    "max_dict = ec.max_dict\n",
    "# then split dataset\n",
    "dump(sc_delay_pipeline, open(pipeline1_file_name,'wb'))\n",
    "dump(sc_delay_pipeline_keras_prep, open(pipeline2_file_name,'wb'))\n",
    "dtrain, dvalid, test = get_train_validation_test(X)\n",
    "# then apply second portion of pipeline to each subset\n",
    "# need to have a distinct object for each to prevent first object impacting others\n",
    "\n",
    "X_train_list = sc_delay_pipeline_keras_prep.fit_transform(dtrain)\n",
    "X_valid_list = sc_delay_pipeline_keras_prep_valid.fit_transform(dvalid)\n",
    "X_test_list = sc_delay_pipeline_keras_prep_test.fit_transform(test)\n",
    "\n",
    "print(\"keras variables defined\")\n",
    "print(\"X_train_list\",X_train_list)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and fit model <a name='modelfit' />\n",
    "\n",
    "- define the architecture of the model - the layers determined by the type (categorical, continuous, or text) of each column\n",
    "- apply callbacks\n",
    "- fit the model\n",
    "\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model in Keras\n",
    "\n",
    "def get_model():\n",
    "    ''' define Keras model by specifying layers by column type\n",
    "    \n",
    "    Returns:\n",
    "        model: Keras model with the layers specified by the structure of the dataset\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif save_model_plot:\\n    model_plot_file = \"model_plot\"+modifier+\".png\"\\n    model_plot_path = os.path.join(get_path(),model_plot_file)\\n    print(\"model plot path: \",model_plot_path)\\n    plot_model(model, to_file=model_plot_path)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save model plot\n",
    "'''\n",
    "if save_model_plot:\n",
    "    model_plot_file = \"model_plot\"+modifier+\".png\"\n",
    "    model_plot_path = os.path.join(get_path(),model_plot_file)\n",
    "    print(\"model plot path: \",model_plot_path)\n",
    "    plot_model(model, to_file=model_plot_path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up early stopping\n",
    "def set_early_stop(es_monitor, es_mode):\n",
    "    ''' given monitoring parameter es_monitor and mode es_mode, define early stopping callback, save model callback, and \n",
    "    TensorBoard callback\n",
    "    \n",
    "    Args:\n",
    "        es_monitor: the performance parameter to monitor in the callback\n",
    "        es_mode: the extremity (max or min) to optimize towards\n",
    "        \n",
    "    Returns:\n",
    "        callback_list: list of callback objects\n",
    "        save_model_path: fully qualified filename to save optimal model to  \n",
    "    \n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"text cols\",textcols)\\nprint(\"dropout \",dropout_rate)\\nprint(\"L2 lambda \",l2_lambda)\\nprint(\"batch size \",batch_size)\\nprint(\"epochs\",epochs)\\nprint(\"learning_rate\",learning_rate)\\nprint(\"loss function\",loss_func)\\nprint(\"output activation function\",output_activation)\\nprint(\"patience_threshold is \",patience_threshold)\\nprint(\"experiment number is \",experiment_number)\\nprint(\"early stop is\",early_stop)\\n# get definitions of callbacks\\ncallback_list, save_model_path = set_early_stop(es_monitor, es_mode)\\n# define model\\nmodel = get_model()\\n# if callbacks are specified \\nif early_stop:\\n       modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\\n        , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1,callbacks=callback_list)\\nelse:\\n    modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\\n         , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# master block to define callbacks & model and fit model \n",
    "'''\n",
    "print(\"text cols\",textcols)\n",
    "print(\"dropout \",dropout_rate)\n",
    "print(\"L2 lambda \",l2_lambda)\n",
    "print(\"batch size \",batch_size)\n",
    "print(\"epochs\",epochs)\n",
    "print(\"learning_rate\",learning_rate)\n",
    "print(\"loss function\",loss_func)\n",
    "print(\"output activation function\",output_activation)\n",
    "print(\"patience_threshold is \",patience_threshold)\n",
    "print(\"experiment number is \",experiment_number)\n",
    "print(\"early stop is\",early_stop)\n",
    "# get definitions of callbacks\n",
    "callback_list, save_model_path = set_early_stop(es_monitor, es_mode)\n",
    "# define model\n",
    "model = get_model()\n",
    "# if callbacks are specified \n",
    "if early_stop:\n",
    "       modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\n",
    "        , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1,callbacks=callback_list)\n",
    "else:\n",
    "    modelfit = model.fit(X_train_list, dtrain.target, epochs=epochs, batch_size=batch_size\n",
    "         , validation_data=(X_valid_list, dvalid.target), class_weight = {0 : zero_weight, 1: one_weight}, verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif early_stop == False:\\n    model_json = model.to_json()\\n    model_path = get_model_path()\\n    with open(os.path.join(model_path,\\'model\\'+modifier+\\'.json\\'), \"w\") as json_file:\\n        json_file.write(model_json)\\n    # serialize weights to HDF5\\n    model.save_weights(os.path.join(model_path,\\'scweights\\'+modifier+\\'.h5\\'))\\n    save_model_path = os.path.join(model_path,\\'scmodel\\'+modifier+\\'.h5\\')\\n    model.save(save_model_path,save_format=\\'h5\\')\\n    # no early stop, so make current model saved_model\\n    saved_model = model\\n    print(\"Saved model, weights to disk\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block to save model elements explicitly if not saved as part of early_stop\n",
    "'''\n",
    "if early_stop == False:\n",
    "    model_json = model.to_json()\n",
    "    model_path = get_model_path()\n",
    "    with open(os.path.join(model_path,'model'+modifier+'.json'), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(os.path.join(model_path,'scweights'+modifier+'.h5'))\n",
    "    save_model_path = os.path.join(model_path,'scmodel'+modifier+'.h5')\n",
    "    model.save(save_model_path,save_format='h5')\n",
    "    # no early stop, so make current model saved_model\n",
    "    saved_model = model\n",
    "    print(\"Saved model, weights to disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsaved_model = load_model(save_model_path)\\nprint(\"metrics names \",saved_model.metrics_names)\\n# getting test and train accuracy is time consuming - check to see if experiment requires it\\nif get_test_train_acc:\\n    if len(saved_model.metrics_names) == 2:\\n        # saved_model.evaluate returns [\\'loss\\', \\'acc\\']\\n        _, train_acc = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\\n        _, test_acc = saved_model.evaluate(X_test_list, test.target, verbose=0)\\n    else:\\n        # saved_model.evaluate returns [\\'loss\\', \\'accuracy\\', \\'accuracy_1\\']\\n        _, train_acc,_ = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\\n        _, test_acc,_ = saved_model.evaluate(X_test_list, test.target, verbose=0)\\n    print(\\'Train: %.3f, Test: %.3f\\' % (train_acc, test_acc))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# block to load saved model and get test and train accuracy\n",
    "'''\n",
    "saved_model = load_model(save_model_path)\n",
    "print(\"metrics names \",saved_model.metrics_names)\n",
    "# getting test and train accuracy is time consuming - check to see if experiment requires it\n",
    "if get_test_train_acc:\n",
    "    if len(saved_model.metrics_names) == 2:\n",
    "        # saved_model.evaluate returns ['loss', 'acc']\n",
    "        _, train_acc = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\n",
    "        _, test_acc = saved_model.evaluate(X_test_list, test.target, verbose=0)\n",
    "    else:\n",
    "        # saved_model.evaluate returns ['loss', 'accuracy', 'accuracy_1']\n",
    "        _, train_acc,_ = saved_model.evaluate(X_train_list, dtrain.target, verbose=0)\n",
    "        _, test_acc,_ = saved_model.evaluate(X_test_list, test.target, verbose=0)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and renderings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npreds = saved_model.predict(X_test_list, batch_size=batch_size)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions on training set\n",
    "'''\n",
    "preds = saved_model.predict(X_test_list, batch_size=batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest[\"predict\"] = preds\\ntest.predict[:5]\\nif verboseout:\\n    test.predict.hist()\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test[\"predict\"] = preds\n",
    "test.predict[:5]\n",
    "if verboseout:\n",
    "    test.predict.hist()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest[\"predround\"] = preds.round().astype(int)\\ntest.predround[:5]\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rounded predictions\n",
    "'''\n",
    "test[\"predround\"] = preds.round().astype(int)\n",
    "test.predround[:5]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredval = model.predict(X_valid_list, batch_size=batch_size)\\ndvalid[\"predround\"] = predval.round().astype(int)\\ndvalid[\"predict\"] = predval\\n#print(type(deltaval))\\n#print(len(deltaval))\\ndvalid.predict[:5]\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict values for validation X values\n",
    "# X_valid, dvalid.target\n",
    "'''\n",
    "predval = model.predict(X_valid_list, batch_size=batch_size)\n",
    "dvalid[\"predround\"] = predval.round().astype(int)\n",
    "dvalid[\"predict\"] = predval\n",
    "#print(type(deltaval))\n",
    "#print(len(deltaval))\n",
    "dvalid.predict[:5]\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndvalid[\"deltaval\"] = abs(dvalid.target - dvalid.predround)\\nprint(dvalid[\"deltaval\"][:10])\\nprint(dvalid[\"deltaval\"].sum())\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hand calculation of proportion correct guesses in validation set\n",
    "'''\n",
    "dvalid[\"deltaval\"] = abs(dvalid.target - dvalid.predround)\n",
    "print(dvalid[\"deltaval\"][:10])\n",
    "print(dvalid[\"deltaval\"].sum())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest[\"deltaval\"] = abs(test.target - test.predround)\\nprint(test[\"deltaval\"][:10])\\nprint(test[\"deltaval\"].sum())\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hand calculation of proportion correct guesses in validation set\n",
    "'''\n",
    "test[\"deltaval\"] = abs(test.target - test.predround)\n",
    "print(test[\"deltaval\"][:10])\n",
    "print(test[\"deltaval\"].sum())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndvalidwrong = dvalid.loc[(dvalid.deltaval == 1)]\\ndvalidright = dvalid.loc[(dvalid.deltaval == 0)]\\ndvalidwrong.head(20)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get subset of dataframe with wrong guesses\n",
    "# k1 = df.loc[(df.Product == p_id)\n",
    "'''\n",
    "dvalidwrong = dvalid.loc[(dvalid.deltaval == 1)]\n",
    "dvalidright = dvalid.loc[(dvalid.deltaval == 0)]\n",
    "dvalidwrong.head(20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntestwrong = test.loc[(test.deltaval == 1)]\\ntestright = test.loc[(test.deltaval == 0)]\\ntestwrong.head(20)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get subset of dataframe with wrong guesses\n",
    "'''\n",
    "testwrong = test.loc[(test.deltaval == 1)]\n",
    "testright = test.loc[(test.deltaval == 0)]\n",
    "testwrong.head(20)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndeltatr = abs(dvalid.target[:100000] - dvalid.predround[:100000])\\ndeltatr[:50]\\nprint(deltatr.sum())\\nprint(\"percentage correct validate\")\\nprint((len(deltatr) - deltatr.sum())/len(deltatr))\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get delta between predictions on training set and actual training target values\n",
    "# hand calculate accuracy on training set as ratio of (total training samples - wrong training predictions)/total training samples\n",
    "'''\n",
    "deltatr = abs(dvalid.target[:100000] - dvalid.predround[:100000])\n",
    "deltatr[:50]\n",
    "print(deltatr.sum())\n",
    "print(\"percentage correct validate\")\n",
    "print((len(deltatr) - deltatr.sum())/len(deltatr))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(modelfit.history.keys())\\n#  acc\\n#plt.plot(modelfit.history['accuracy'])\\nplt.plot(modelfit.history['val_accuracy'])\\nplt.title('model accuracy')\\nplt.ylabel('accuracy')\\nplt.xlabel('epoch')\\n# plt.legend(['train', 'validation'], loc='upper left')\\n#plt.legend(['validation'], loc='upper left')\\nplt.show()\\n# Loss\\nplt.plot(modelfit.history['loss'])\\nplt.plot(modelfit.history['val_loss'])\\nplt.title('model loss')\\nplt.ylabel('loss')\\nplt.xlabel('epoch')\\nplt.legend(['train', 'validation'], loc='upper left')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chart accuracy and loss for train and validation sets\n",
    "'''\n",
    "print(modelfit.history.keys())\n",
    "#  acc\n",
    "#plt.plot(modelfit.history['accuracy'])\n",
    "plt.plot(modelfit.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.legend(['validation'], loc='upper left')\n",
    "plt.show()\n",
    "# Loss\n",
    "plt.plot(modelfit.history['loss'])\n",
    "plt.plot(modelfit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix <a name='confusionmatrix' />\n",
    "<a href=#linkanchor>Back to link list</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ncfmap=metrics.confusion_matrix(y_true=test[\\'target\\'],  # True labels\\n                         y_pred=test[\"predround\"])\\n\\nlabel = [\"0\", \"1\"]\\nsns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\\nplt.xlabel(\"Prediction\")\\nplt.title(\"Confusion Matrix for Airbnb NYC price prediction (weighted)\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "cfmap=metrics.confusion_matrix(y_true=test['target'],  # True labels\n",
    "                         y_pred=test[\"predround\"])\n",
    "\n",
    "label = [\"0\", \"1\"]\n",
    "sns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.title(\"Confusion Matrix for Airbnb NYC price prediction (weighted)\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle submission that was used as input for this notebook\n",
    "https://www.kaggle.com/knowledgegrappler/a-simple-nn-solution-with-keras-0-48611-pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook shows methods for dealing with structured data in the context of a neural network.\n",
    "\n",
    "# Author\n",
    "\n",
    "Mark Ryan is a manager at Intact Insurance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "try_tf2",
   "language": "python",
   "name": "try_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
