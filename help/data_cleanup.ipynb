{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Airbnb Price Prediction - Data Cleanup - HELP VERSION\n",
    "\n",
    "Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple deep learning model to predict prices for Airbnb properties.\n",
    "\n",
    "\n",
    "This notebook contains the data cleanup steps:\n",
    "- load data from the input CSV or dataframe saved by the [data preparation notebook](https://github.com/ryanmark1867/end_to_end_deep_learning_liveproject/blob/master/notebooks/data_preparation.ipynb)\n",
    "- fix missing values\n",
    "- clean up anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common imports and variables\n",
    "Imports and variable definitions that are common to the entire notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 21.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\ryanm\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 21.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "# common imports\n",
    "import zipfile\n",
    "import time\n",
    "# import datetime, timedelta\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "from dateutil import relativedelta\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from io import StringIO\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import os\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from IPython.display import display\n",
    "import logging\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(config_file):\n",
    "    ''' open config file with name config_file that contains parameters\n",
    "    for this module and return Python object\n",
    "\n",
    "    Args:\n",
    "        config_file: filename containing config parameters\n",
    "\n",
    "    Returns:\n",
    "        config: Python dictionary with config parms from config file - dictionary\n",
    "\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_values(config):\n",
    "    ''' print values in dictionary config'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataframe\n",
    "- load pickled dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    ''' get the path for data files\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(path,input_csv,pickled_input_dataframe,save_raw_dataframe,load_from_scratch):\n",
    "    ''' load data into dataframe\n",
    "    Args:\n",
    "        path: path containing input file\n",
    "        input_csv: input file name\n",
    "        pickled_input_dataframe: pickled version of input file\n",
    "\n",
    "    Returns:\n",
    "        path: path for data directory\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(dataset,columns,defaults):\n",
    "    ''' replace missing values with placeholders by column type\n",
    "    \n",
    "    Args:\n",
    "        dataset: dataframe in which missing values being processed\n",
    "        columns: dictionary of columns with keys that are column types and values that are the column names of that type\n",
    "        defaults: dictionary of replacement values for missing values by column type\n",
    "\n",
    "    Returns:\n",
    "        dataset: dataframe with missing values replaced with default values\n",
    "\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in_list(x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        retur_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_val(x):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_assessment(df,columns,valid_values,non_neg_continuous):\n",
    "    ''' assess the values in a dataframe\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_range(x,max,min):\n",
    "    ''' count whether a value is in a range\n",
    "    Args:\n",
    "        x: value to check in range\n",
    "        max: top of the range to check\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_of_bounding_box(latitude,longitude,bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "        min: bottom of the range to check\n",
    "        \n",
    "    Returns:\n",
    "        ret_val: 1 if out of range, 0 otherwise\n",
    "    '''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_assessment(df,bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    ''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_invalid_values(df,columns,valid_values,invalid_value_replacements,non_neg_continuous):\n",
    "    ''' replace invalid with placeholders\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        columns: dictionary of column names by category\n",
    "        valid_values: dictionary of valid values for categorical columns with limited number of valid values\n",
    "        invalid_value_replacements: dictionary of replacement values by column category\n",
    "        non_neg_continuous: list of continuous columns with only non-negative values as valid\n",
    "    Returns:\n",
    "        df: updated dataframe\n",
    "    '''\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_non_numeric(x, replace_x):\n",
    "    ''' check if a value is non-numeric and replace if so\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_not_in_list(x, replace_x, list):\n",
    "    ''' check if a value is in a list\n",
    "    Args:\n",
    "        x: value to check\n",
    "        list: list in which to check for the value\n",
    "\n",
    "    Returns:\n",
    "        retur_val: 1 if value is in not in list, 0 otherwise\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_neg(x, replace_x):\n",
    "    ''' check if a value is negative and replace if so\n",
    "    Args:\n",
    "        x: value to check\n",
    "    \n",
    "    Returns:\n",
    "        retur_val: 1 if value is negative, 0 otherwise\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_replacement(df,replace_lat, replace_long, bounding_box):\n",
    "    ''' assess the geo columns in a dataframe by counting how many latitude and longitude values are outside the bounding box\n",
    "    Args:\n",
    "        df: dataframe for assessment\n",
    "        replace_lat: replacement for latitude if out of bounds\n",
    "        replace_long: replacement for longitude if out of bounds\n",
    "        bounding_box: dictionary of maximum and minimum valid latitude and longitude values\n",
    "    Returns:\n",
    "        df: updated dataframe\n",
    "    ''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_if_outside_bounding_box(latitude,longitude, replace_lat, replace_long, bounding_box):\n",
    "    ''' count whether a location is within a bounding box\n",
    "    Args:\n",
    "        latitude: latitude portion of location\n",
    "        longitude: longitude portion of location\n",
    "        replace_lat: replacement value for latitude\n",
    "        replace_long: replacement value for longitude\n",
    "        bounding_box: dictionary with max and min values to compare the location with\n",
    "               \n",
    "    Returns:\n",
    "        latitude, longitude: 1 if out of range, 0 otherwise\n",
    "    '''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_time(date_time_value,time_value):\n",
    "    ''' given a datetime replace the time portion '''\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master cell\n",
    "This cell contains calls to the other functions in this notebook to complete the data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npath = get_path()\\nprint(\"path is \",path)\\nconfig = get_config(\\'data_preparation_config.yml\\')\\nprint(\"past config definition\")\\nlogging.getLogger().setLevel(logging.WARNING)\\nlogging.warning(\"logging check\")\\nprint_config_values(config)\\n# load dataframe\\ndf = ingest_data(path,config[\\'file_names\\'][\\'input_csv\\'],config[\\'file_names\\'][\\'pickled_input_dataframe\\'],config[\\'general\\'][\\'save_raw_dataframe\\'],config[\\'general\\'][\\'load_from_scratch\\'])\\nprint(\"columns is \"+str(config[\\'columns\\']))\\nprint(\"category_defaults is \"+str(config[\\'category_defaults\\']))\\n# fill missing values according to the defaults per column\\ndf = fill_missing(df,config[\\'columns\\'],config[\\'category_defaults\\'])\\n# get assessment results after filling missing values\\nbasic_assessment(df,config[\\'columns\\'],config[\\'valid_values\\'],config[\\'non_negative_continuous\\'])\\n# df = replace_invalid_values(df,columns,valid_values,invalid_value_replacements,non_neg_continuous)\\ndf = replace_invalid_values(df,config[\\'columns\\'],config[\\'valid_values\\'],config[\\'category_invalid_value_replacements\\'],config[\\'non_negative_continuous\\'])\\ndf = geo_replacement(df,config[\\'latitude_replacement\\'],config[\\'longitude_replacement\\'], config[\\'bounding_box\\'])\\ngeo_assessment(df,config[\\'bounding_box\\'])\\nif config[\\'general\\'][\\'save_transformed_dataframe\\']:\\n    print(\"path is \",path)    \\n    file_name = os.path.join(path,config[\\'file_names\\'][\\'pickled_output_dataframe\\'])\\n    print(\"file_name is \",file_name)\\n    df.to_pickle(file_name)\\ndf.head()\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master cell to call the other functions\n",
    "# get the path for data files\n",
    "\n",
    "'''\n",
    "path = get_path()\n",
    "print(\"path is \",path)\n",
    "config = get_config('data_preparation_config.yml')\n",
    "print(\"past config definition\")\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.warning(\"logging check\")\n",
    "print_config_values(config)\n",
    "# load dataframe\n",
    "df = ingest_data(path,config['file_names']['input_csv'],config['file_names']['pickled_input_dataframe'],config['general']['save_raw_dataframe'],config['general']['load_from_scratch'])\n",
    "print(\"columns is \"+str(config['columns']))\n",
    "print(\"category_defaults is \"+str(config['category_defaults']))\n",
    "# fill missing values according to the defaults per column\n",
    "df = fill_missing(df,config['columns'],config['category_defaults'])\n",
    "# get assessment results after filling missing values\n",
    "basic_assessment(df,config['columns'],config['valid_values'],config['non_negative_continuous'])\n",
    "# df = replace_invalid_values(df,columns,valid_values,invalid_value_replacements,non_neg_continuous)\n",
    "df = replace_invalid_values(df,config['columns'],config['valid_values'],config['category_invalid_value_replacements'],config['non_negative_continuous'])\n",
    "df = geo_replacement(df,config['latitude_replacement'],config['longitude_replacement'], config['bounding_box'])\n",
    "geo_assessment(df,config['bounding_box'])\n",
    "if config['general']['save_transformed_dataframe']:\n",
    "    print(\"path is \",path)    \n",
    "    file_name = os.path.join(path,config['file_names']['pickled_output_dataframe'])\n",
    "    print(\"file_name is \",file_name)\n",
    "    df.to_pickle(file_name)\n",
    "df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
